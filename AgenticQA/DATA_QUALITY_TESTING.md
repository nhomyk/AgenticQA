# Data Quality Testing - Ensuring Consistency Across Deployments

Comprehensive data quality testing suite ensuring data integrity, consistency, and reliability across all deployments.

## ğŸ¯ Overview

The data quality testing system validates:
- âœ… Artifact integrity with SHA256 checksums
- âœ… Schema consistency across all metadata
- âœ… No duplicate artifacts
- âœ… Metadata completeness
- âœ… Index accuracy
- âœ… Data immutability (same on every read)
- âœ… PII protection and scanning
- âœ… Temporal consistency (no future dates, reasonable timeframes)
- âœ… Cross-deployment consistency
- âœ… Deployment readiness

## ğŸ“Š Test Suite

### 10 Comprehensive Tests

1. **Artifact Integrity** - Verify all artifacts pass integrity checks
2. **Checksum Validation** - Validate SHA256 checksums on all artifacts
3. **Schema Consistency** - Ensure consistent metadata schema
4. **No Duplicates** - Verify no duplicate artifact IDs
5. **Metadata Completeness** - All required metadata fields present
6. **Index Accuracy** - Index matches actual artifacts
7. **Data Immutability** - Data unchanged on multiple reads
8. **PII Protection** - No exposed personal information
9. **Temporal Consistency** - Timestamps are valid and reasonable
10. **Cross-Deployment Consistency** - Consistency for multi-environment reliability

## ğŸš€ Pipeline Integration

### Pre-Execution Data Quality Check

Validates input data BEFORE agents process it:

```python
from src.data_store.data_quality_pipeline import DataQualityValidatedPipeline

pipeline = DataQualityValidatedPipeline()

# Pre-execution validation
input_data = {
    "timestamp": "2026-01-23T10:00:00Z",
    "data": {"test_count": 150}
}

is_valid, result = pipeline.validate_input_data("QA_Assistant", input_data)
# Runs: schema validation, PII check, encryption readiness, snapshots data
```

### Post-Execution Data Quality Check

Validates output data AFTER agents process it:

```python
# Post-execution validation
execution_result = {
    "timestamp": "2026-01-23T10:00:00Z",
    "agent_name": "QA_Assistant",
    "status": "success",
    "output": {...}
}

is_valid, result = pipeline.execute_with_validation("QA_Assistant", execution_result)
# Runs: all 10 data quality tests, snapshot comparison, exports results
```

### Deployment Validation

Complete validation before deploying:

```python
deployment_result = pipeline.run_deployment_validation()
# Returns: ready_for_deployment boolean and detailed audit trail
```

## ğŸ“ˆ Data Flow with Quality Testing

```
Input Data
    â†“
[Pre-Execution Quality Checks]
    â†“
Agent Execution
    â†“
[Post-Execution Quality Tests - All 10 Tests]
    â†“
Data Storage & Integrity Verification
    â†“
Quality Report Export
    â†“
Ready for Deployment?
```

## ğŸ” Data Quality Test Examples

```python
from src.data_store.data_quality_pipeline import DataQualityValidatedPipeline

# Create pipeline with quality testing enabled
pipeline = DataQualityValidatedPipeline(run_quality_tests=True)

# 1. Pre-execution quality check
is_valid, result = pipeline.validate_input_data("agent", input_data)
if "quality_tests" in result:
    quality = result["quality_tests"]
    print(f"Quality: {quality['summary']['passed']}/{quality['summary']['total_tests']} passed")

# 2. Post-execution quality check
is_valid, result = pipeline.execute_with_validation("agent", execution_result)
if "post_execution_quality" in result:
    quality = result["post_execution_quality"]
    if quality["summary"]["all_passed"]:
        print("âœ“ All data quality tests passed!")
    else:
        print(f"âœ— {quality['summary']['failed']} tests failed")

# 3. Deployment validation
deployment = pipeline.run_deployment_validation()
if deployment["ready_for_deployment"]:
    print("âœ“ Ready for deployment to production")
else:
    print("âœ— Not ready for deployment")
```

## ğŸ“‹ Quality Test Results Export

All test results are automatically exported to:
- `.test-artifact-store/patterns/data_quality_test_results.json`

Contains:
- Timestamp of test run
- Individual test results with pass/fail status
- Detailed failure messages
- Summary statistics
- Deployment readiness determination

## ğŸ›¡ï¸ Security in Quality Testing

- âœ… PII scanning with regex patterns (email, SSN, credit card, API keys)
- âœ… Checksum validation prevents tampering detection
- âœ… Immutability testing ensures data hasn't been modified
- âœ… Schema validation prevents data corruption
- âœ… Cross-deployment consistency ensures replicas match

## ğŸ“Š Deployment Readiness Criteria

Deployment is ready when:
1. âœ“ All 10 data quality tests pass
2. âœ“ All artifacts are accessible
3. âœ“ Pattern analysis completes successfully
4. âœ“ No PII leakage detected
5. âœ“ Checksums validate on 100% of artifacts

## ğŸš€ Integration with Agents

Agents automatically use quality-validated pipeline:

```python
from src.agents import QAAssistantAgent

# Agent automatically runs quality checks
qa_agent = QAAssistantAgent()
result = qa_agent.execute(test_results)
# Runs pre/post quality validation automatically
```

## ğŸ“ˆ Continuous Quality Assurance

```python
# Run continuous quality checks
pipeline = DataQualityValidatedPipeline()
deployment_result = pipeline.run_deployment_validation()

if deployment_result["ready_for_deployment"]:
    # Safe to deploy
    deploy_to_production()
else:
    # Show failures and fix before deploying
    print(deployment_result["checks"]["data_quality"]["tests"])
```

## ğŸ¯ For Production

This quality testing suite ensures:
- ğŸ”’ **Data Integrity**: SHA256 checksums prevent tampering
- ğŸ“Š **Consistency**: All deployments see identical data
- ğŸ›¡ï¸ **Security**: PII protection and leak detection
- âœ“ **Reliability**: 10-point validation before deployment
- ğŸ“ˆ **Audit Trail**: Complete test history for compliance
- ğŸš€ **Confidence**: Deploy with certainty across environments

Run examples:
```bash
python example_data_quality_testing.py
```
